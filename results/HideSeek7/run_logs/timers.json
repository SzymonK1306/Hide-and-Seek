{
    "name": "root",
    "gauges": {
        "HiderBehavior.Policy.Entropy.mean": {
            "value": 4.731152534484863,
            "min": 1.418938398361206,
            "max": 4.7311530113220215,
            "count": 453
        },
        "HiderBehavior.Policy.Entropy.sum": {
            "value": 426475.5625,
            "min": 126484.171875,
            "max": 427279.84375,
            "count": 453
        },
        "HiderBehavior.Step.mean": {
            "value": 40769992.0,
            "min": 89984.0,
            "max": 40769992.0,
            "count": 453
        },
        "HiderBehavior.Step.sum": {
            "value": 40769992.0,
            "min": 89984.0,
            "max": 40769992.0,
            "count": 453
        },
        "HiderBehavior.Policy.ExtrinsicBaselineEstimate.mean": {
            "value": 23.050703048706055,
            "min": 0.1563398838043213,
            "max": 29.48441505432129,
            "count": 453
        },
        "HiderBehavior.Policy.ExtrinsicBaselineEstimate.sum": {
            "value": 33469.62109375,
            "min": 225.2857666015625,
            "max": 43017.76171875,
            "count": 453
        },
        "HiderBehavior.Policy.ExtrinsicValueEstimate.mean": {
            "value": 23.43590545654297,
            "min": 0.1664654016494751,
            "max": 28.151165008544922,
            "count": 453
        },
        "HiderBehavior.Policy.ExtrinsicValueEstimate.sum": {
            "value": 34028.93359375,
            "min": 239.87664794921875,
            "max": 41072.55078125,
            "count": 453
        },
        "HiderBehavior.Environment.EpisodeLength.mean": {
            "value": 466.80526315789473,
            "min": 441.26237623762376,
            "max": 496.84444444444443,
            "count": 453
        },
        "HiderBehavior.Environment.EpisodeLength.sum": {
            "value": 88693.0,
            "min": 85913.0,
            "max": 95437.0,
            "count": 453
        },
        "HiderBehavior.Environment.CumulativeReward.mean": {
            "value": 3.568525824107622,
            "min": 1.4694419112470416,
            "max": 4.206783603779308,
            "count": 453
        },
        "HiderBehavior.Environment.CumulativeReward.sum": {
            "value": 678.0199065804482,
            "min": 264.49954402446747,
            "max": 803.4956683218479,
            "count": 453
        },
        "HiderBehavior.Policy.ExtrinsicReward.mean": {
            "value": 71.06172827701819,
            "min": 38.367241168886885,
            "max": 96.20058920234442,
            "count": 453
        },
        "HiderBehavior.Policy.ExtrinsicReward.sum": {
            "value": 13501.728372633457,
            "min": 7404.877545595169,
            "max": 17509.56130003929,
            "count": 453
        },
        "HiderBehavior.Environment.GroupCumulativeReward.mean": {
            "value": 78.34394904458598,
            "min": 50.0,
            "max": 96.55172413793103,
            "count": 453
        },
        "HiderBehavior.Environment.GroupCumulativeReward.sum": {
            "value": 12300.0,
            "min": 6800.0,
            "max": 16900.0,
            "count": 453
        },
        "HiderBehavior.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 453
        },
        "HiderBehavior.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 453
        },
        "SeekerBehavior.Policy.Entropy.mean": {
            "value": 2.527959108352661,
            "min": 1.418938159942627,
            "max": 2.527959108352661,
            "count": 85
        },
        "SeekerBehavior.Policy.Entropy.sum": {
            "value": 379143.3125,
            "min": 212074.5,
            "max": 379143.3125,
            "count": 85
        },
        "SeekerBehavior.Step.mean": {
            "value": 12749991.0,
            "min": 149954.0,
            "max": 12749991.0,
            "count": 85
        },
        "SeekerBehavior.Step.sum": {
            "value": 12749991.0,
            "min": 149954.0,
            "max": 12749991.0,
            "count": 85
        },
        "SeekerBehavior.Policy.ExtrinsicBaselineEstimate.mean": {
            "value": -11.8203763961792,
            "min": -11.8203763961792,
            "max": 0.10090479254722595,
            "count": 85
        },
        "SeekerBehavior.Policy.ExtrinsicBaselineEstimate.sum": {
            "value": -29539.12109375,
            "min": -29539.12109375,
            "max": 251.9592742919922,
            "count": 85
        },
        "SeekerBehavior.Policy.ExtrinsicValueEstimate.mean": {
            "value": -11.8203763961792,
            "min": -11.8203763961792,
            "max": 0.10090479254722595,
            "count": 85
        },
        "SeekerBehavior.Policy.ExtrinsicValueEstimate.sum": {
            "value": -29539.12109375,
            "min": -29539.12109375,
            "max": 251.9592742919922,
            "count": 85
        },
        "SeekerBehavior.Environment.EpisodeLength.mean": {
            "value": 294.6188605108055,
            "min": 285.64312977099235,
            "max": 299.48080808080806,
            "count": 85
        },
        "SeekerBehavior.Environment.EpisodeLength.sum": {
            "value": 149961.0,
            "min": 147989.0,
            "max": 150883.0,
            "count": 85
        },
        "SeekerBehavior.Environment.CumulativeReward.mean": {
            "value": 3.8737097687535282,
            "min": 3.707737242522305,
            "max": 4.202091608996911,
            "count": 85
        },
        "SeekerBehavior.Environment.CumulativeReward.sum": {
            "value": 1967.8445625267923,
            "min": 1894.6537309288979,
            "max": 2100.6669761883095,
            "count": 85
        },
        "SeekerBehavior.Policy.ExtrinsicReward.mean": {
            "value": -61.28377036043391,
            "min": -86.020797430024,
            "max": -25.77089274752708,
            "count": 85
        },
        "SeekerBehavior.Policy.ExtrinsicReward.sum": {
            "value": -31132.15534310043,
            "min": -42580.29472786188,
            "max": -13529.718692451715,
            "count": 85
        },
        "SeekerBehavior.Environment.GroupCumulativeReward.mean": {
            "value": -65.15748031496064,
            "min": -89.8989898989899,
            "max": -29.714285714285715,
            "count": 85
        },
        "SeekerBehavior.Environment.GroupCumulativeReward.sum": {
            "value": -33100.0,
            "min": -44500.0,
            "max": -15600.0,
            "count": 85
        },
        "SeekerBehavior.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 85
        },
        "SeekerBehavior.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 85
        },
        "HiderBehavior.Losses.PolicyLoss.mean": {
            "value": 0.02346258118869931,
            "min": 0.02346258118869931,
            "max": 0.03155529197024104,
            "count": 43
        },
        "HiderBehavior.Losses.PolicyLoss.sum": {
            "value": 0.02346258118869931,
            "min": 0.02346258118869931,
            "max": 0.03155529197024104,
            "count": 43
        },
        "HiderBehavior.Losses.ValueLoss.mean": {
            "value": 79.42657299041748,
            "min": 60.393040555318194,
            "max": 146.8833563232422,
            "count": 43
        },
        "HiderBehavior.Losses.ValueLoss.sum": {
            "value": 79.42657299041748,
            "min": 60.393040555318194,
            "max": 146.8833563232422,
            "count": 43
        },
        "HiderBehavior.Losses.BaselineLoss.mean": {
            "value": 88.16473997751872,
            "min": 79.50986423492432,
            "max": 230.63586049397787,
            "count": 43
        },
        "HiderBehavior.Losses.BaselineLoss.sum": {
            "value": 88.16473997751872,
            "min": 79.50986423492432,
            "max": 230.63586049397787,
            "count": 43
        },
        "HiderBehavior.Policy.LearningRate.mean": {
            "value": 0.0002976054627181793,
            "min": 0.0002976054627181793,
            "max": 0.00029994383371872215,
            "count": 43
        },
        "HiderBehavior.Policy.LearningRate.sum": {
            "value": 0.0002976054627181793,
            "min": 0.0002976054627181793,
            "max": 0.00029994383371872215,
            "count": 43
        },
        "HiderBehavior.Policy.Epsilon.mean": {
            "value": 0.19920182064,
            "min": 0.19920182064,
            "max": 0.19998127790000006,
            "count": 43
        },
        "HiderBehavior.Policy.Epsilon.sum": {
            "value": 0.19920182064,
            "min": 0.19920182064,
            "max": 0.19998127790000006,
            "count": 43
        },
        "HiderBehavior.Policy.Beta.mean": {
            "value": 0.09920190045793599,
            "min": 0.09920190045793599,
            "max": 0.09998127977221002,
            "count": 43
        },
        "HiderBehavior.Policy.Beta.sum": {
            "value": 0.09920190045793599,
            "min": 0.09920190045793599,
            "max": 0.09998127977221002,
            "count": 43
        },
        "SeekerBehavior.Losses.PolicyLoss.mean": {
            "value": 0.02496641832583312,
            "min": 0.022737523364121444,
            "max": 0.02731324816973938,
            "count": 14
        },
        "SeekerBehavior.Losses.PolicyLoss.sum": {
            "value": 0.02496641832583312,
            "min": 0.022737523364121444,
            "max": 0.02731324816973938,
            "count": 14
        },
        "SeekerBehavior.Losses.ValueLoss.mean": {
            "value": 155.78033498128255,
            "min": 149.19426846822103,
            "max": 219.02439224243165,
            "count": 14
        },
        "SeekerBehavior.Losses.ValueLoss.sum": {
            "value": 155.78033498128255,
            "min": 149.19426846822103,
            "max": 219.02439224243165,
            "count": 14
        },
        "SeekerBehavior.Losses.BaselineLoss.mean": {
            "value": 176.5721058654785,
            "min": 168.46131268819173,
            "max": 277.7765054829915,
            "count": 14
        },
        "SeekerBehavior.Losses.BaselineLoss.sum": {
            "value": 176.5721058654785,
            "min": 168.46131268819173,
            "max": 277.7765054829915,
            "count": 14
        },
        "SeekerBehavior.Policy.LearningRate.mean": {
            "value": 0.0002992449958316681,
            "min": 0.0002992449958316681,
            "max": 0.0002999458748580417,
            "count": 14
        },
        "SeekerBehavior.Policy.LearningRate.sum": {
            "value": 0.0002992449958316681,
            "min": 0.0002992449958316681,
            "max": 0.0002999458748580417,
            "count": 14
        },
        "SeekerBehavior.Policy.Epsilon.mean": {
            "value": 0.19974833186,
            "min": 0.19974833186,
            "max": 0.19998195827999998,
            "count": 14
        },
        "SeekerBehavior.Policy.Epsilon.sum": {
            "value": 0.19974833186,
            "min": 0.19974833186,
            "max": 0.19998195827999998,
            "count": 14
        },
        "SeekerBehavior.Policy.Beta.mean": {
            "value": 0.099748357026814,
            "min": 0.099748357026814,
            "max": 0.09998196008417203,
            "count": 14
        },
        "SeekerBehavior.Policy.Beta.sum": {
            "value": 0.099748357026814,
            "min": 0.099748357026814,
            "max": 0.09998196008417203,
            "count": 14
        }
    },
    "metadata": {
        "timer_format_version": "0.1.0",
        "start_time_seconds": "1697273223",
        "python_version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]",
        "command_line_arguments": "D:\\Programs\\Unity\\Projects\\Engineering\\venv\\Scripts\\mlagents-learn config/new_trainer_config.yaml --run-id=HideSeek7 --torch-device=cuda",
        "mlagents_version": "0.30.0",
        "mlagents_envs_version": "0.30.0",
        "communication_protocol_version": "1.5.0",
        "pytorch_version": "2.0.1+cu117",
        "numpy_version": "1.21.2",
        "end_time_seconds": "1697356185"
    },
    "total": 82962.1577281,
    "count": 1,
    "self": 0.017699300005915575,
    "children": {
        "run_training.setup": {
            "total": 0.11534849999999963,
            "count": 1,
            "self": 0.11534849999999963
        },
        "TrainerController.start_learning": {
            "total": 82962.0246803,
            "count": 1,
            "self": 33.683725901166326,
            "children": {
                "TrainerController._reset_env": {
                    "total": 25.824268099999998,
                    "count": 1,
                    "self": 25.824268099999998
                },
                "TrainerController.advance": {
                    "total": 82901.46997659883,
                    "count": 1518152,
                    "self": 39.58647510125593,
                    "children": {
                        "env_step": {
                            "total": 33172.03049599441,
                            "count": 1518152,
                            "self": 23078.599666199094,
                            "children": {
                                "SubprocessEnvManager._take_step": {
                                    "total": 10075.550415896882,
                                    "count": 1518152,
                                    "self": 620.6369334965693,
                                    "children": {
                                        "TorchPolicy.evaluate": {
                                            "total": 9454.913482400312,
                                            "count": 2902102,
                                            "self": 9454.913482400312
                                        }
                                    }
                                },
                                "workers": {
                                    "total": 17.880413898437745,
                                    "count": 1518152,
                                    "self": 0.0,
                                    "children": {
                                        "worker_root": {
                                            "total": 82685.64276200124,
                                            "count": 1518152,
                                            "is_parallel": true,
                                            "self": 66587.46272010729,
                                            "children": {
                                                "steps_from_proto": {
                                                    "total": 0.001646299999997325,
                                                    "count": 2,
                                                    "is_parallel": true,
                                                    "self": 0.0002748999999901969,
                                                    "children": {
                                                        "_process_rank_one_or_two_observation": {
                                                            "total": 0.001371400000007128,
                                                            "count": 8,
                                                            "is_parallel": true,
                                                            "self": 0.001371400000007128
                                                        }
                                                    }
                                                },
                                                "UnityEnvironment.step": {
                                                    "total": 16098.178395593946,
                                                    "count": 1518152,
                                                    "is_parallel": true,
                                                    "self": 535.3049745785574,
                                                    "children": {
                                                        "UnityEnvironment._generate_step_input": {
                                                            "total": 617.8655859025998,
                                                            "count": 1518152,
                                                            "is_parallel": true,
                                                            "self": 617.8655859025998
                                                        },
                                                        "communicator.exchange": {
                                                            "total": 13604.441794005132,
                                                            "count": 1518152,
                                                            "is_parallel": true,
                                                            "self": 13604.441794005132
                                                        },
                                                        "steps_from_proto": {
                                                            "total": 1340.566041107656,
                                                            "count": 3036304,
                                                            "is_parallel": true,
                                                            "self": 290.15669600383717,
                                                            "children": {
                                                                "_process_rank_one_or_two_observation": {
                                                                    "total": 1050.4093451038189,
                                                                    "count": 12145216,
                                                                    "is_parallel": true,
                                                                    "self": 1050.4093451038189
                                                                }
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        },
                        "trainer_advance": {
                            "total": 49689.853005503166,
                            "count": 3036304,
                            "self": 95.28162700235407,
                            "children": {
                                "process_trajectory": {
                                    "total": 22321.748862000826,
                                    "count": 3036304,
                                    "self": 22271.20660620086,
                                    "children": {
                                        "RLTrainer._checkpoint": {
                                            "total": 50.5422557999658,
                                            "count": 106,
                                            "self": 50.5422557999658
                                        }
                                    }
                                },
                                "_update_policy": {
                                    "total": 27272.82251649999,
                                    "count": 58,
                                    "self": 11088.775209001624,
                                    "children": {
                                        "TorchPOCAOptimizer.update": {
                                            "total": 16184.047307498366,
                                            "count": 17248,
                                            "self": 16184.047307498366
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "trainer_threads": {
                    "total": 1.00000761449337e-06,
                    "count": 1,
                    "self": 1.00000761449337e-06
                },
                "TrainerController._save_models": {
                    "total": 1.0467086999997264,
                    "count": 1,
                    "self": 0.07773950000409968,
                    "children": {
                        "RLTrainer._checkpoint": {
                            "total": 0.9689691999956267,
                            "count": 2,
                            "self": 0.9689691999956267
                        }
                    }
                }
            }
        }
    }
}